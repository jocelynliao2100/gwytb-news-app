import streamlit as st
import pandas as pd
import jieba
import jieba.analyse
from docx import Document
from bs4 import BeautifulSoup
import re
from datetime import datetime
from collections import Counter

# è‡ªè¨‚é—œéµå­—ç¾¤çµ„
KEYWORD_CATEGORIES = {
    "å°ç¨ç›¸é—œ": ["å°ç¨", "æ°‘é€²é»¨", "è³´æ¸…å¾·", "è”¡è‹±æ–‡", "å°ç‹¬", "æ°‘è¿›å…š", "èµ–æ¸…å¾·", "è”¡è‹±æ–‡"],
    "åœ‹æ°‘é»¨ç›¸é—œ": ["åœ‹æ°‘é»¨", "é¦¬è‹±ä¹"],
    "ç¾åœ‹ç›¸é—œ": ["ç¾åœ‹", "ç¾æ–¹", "ç¾å›½"],
    "ç¶“æ¿Ÿç™¼å±•èˆ‡äº¤æµç›¸é—œ": ["ç™¼å±•", "å°å•†", "é’å¹´"],
    "å…¶ä»–": ["ä¹äºŒå…±è­˜", "å…©åœ‹è«–", "2758"]
}

ALL_KEYWORDS = [kw for group in KEYWORD_CATEGORIES.values() for kw in group]


def render_keywords_analysis():
    st.title("ğŸ” åœ‹å°è¾¦æ–°èå…§å®¹é—œéµå­—åˆ†æï¼ˆHTML Word æª”ï¼‰")

    uploaded_files = st.file_uploader(
        "ğŸ“‚ è«‹ä¸Šå‚³å¤šå€‹ HTML åŸå§‹ç¢¼æ ¼å¼çš„ Word æª”ï¼ˆ.docxï¼‰",
        type="docx",
        accept_multiple_files=True
    )

    if uploaded_files:
        records = []

        for file in uploaded_files:
            doc = Document(file)
            html = "\n".join(p.text for p in doc.paragraphs)
            soup = BeautifulSoup(html, "html.parser")

            for li in soup.find_all("li"):
                date_tag = li.find("span")
                link_tag = li.find("a")
                if date_tag and link_tag:
                    date = date_tag.text.strip("[] ")
                    content = link_tag.get("title", "").strip()
                    if re.match(r"\d{4}-\d{2}-\d{2}", date) and content:
                        records.append({"æ—¥æœŸ": date, "å…§å®¹": content})

        df = pd.DataFrame(records)
        df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors="coerce")
        df = df.dropna(subset=["æ—¥æœŸ"])
        df["æœˆä»½"] = df["æ—¥æœŸ"].dt.to_period("M").dt.to_timestamp()

        st.success(f"âœ… æˆåŠŸè¼‰å…¥ {len(df)} å‰‡æ–°èå…§å®¹")
        st.dataframe(df)

        # å…¨æ–‡é—œéµå­—çµ±è¨ˆ
        st.markdown("### ğŸ”  æ‰€æœ‰æ–°èå…§å®¹é—œéµå­—ï¼ˆTop 20ï¼‰")
        full_text = " ".join(df["å…§å®¹"].tolist())
        top_keywords = jieba.analyse.extract_tags(full_text, topK=20, withWeight=True)
        keyword_df = pd.DataFrame(top_keywords, columns=["é—œéµå­—", "æ¬Šé‡"])
        st.bar_chart(keyword_df.set_index("é—œéµå­—")[["æ¬Šé‡"]].rename(columns={"æ¬Šé‡": "æ•¸é‡"}))

        # é—œéµå­—ç¾¤çµ„åˆ†æ
        st.markdown("### ğŸ“Š æŒ‡å®šé—œéµå­—ç¾¤çµ„å‡ºç¾è¶¨å‹¢åˆ†æ")

        keyword_monthly = []
        for month, group in df.groupby("æœˆä»½"):
            contents = " ".join(group["å…§å®¹"].tolist())
            for keyword in ALL_KEYWORDS:
                count = contents.count(keyword)
                keyword_monthly.append({"æœˆä»½": month, "é—œéµå­—": keyword, "å‡ºç¾æ¬¡æ•¸": count})

        km_df = pd.DataFrame(keyword_monthly)

        for group_name, keywords in KEYWORD_CATEGORIES.items():
            st.subheader(f"ğŸ“Œ {group_name}")
            filtered = km_df[km_df["é—œéµå­—"].isin(keywords)]
            pivot = filtered.drop_duplicates(subset=["æœˆä»½", "é—œéµå­—"]).pivot(index="æœˆä»½", columns="é—œéµå­—", values="å‡ºç¾æ¬¡æ•¸").fillna(0)
            st.line_chart(pivot)

            for keyword in keywords:
                subset = km_df[km_df["é—œéµå­—"] == keyword]
                top_months = subset.sort_values("å‡ºç¾æ¬¡æ•¸", ascending=False).head(5)
                st.markdown(f"**ğŸ” {keyword}ï¼šå‡ºç¾æ¬¡æ•¸æœ€å¤šçš„å‰äº”å€‹æœˆä»½ï¼š**")
                st.table(top_months[["æœˆä»½", "å‡ºç¾æ¬¡æ•¸"]])

                for _, row in top_months.iterrows():
                    m = row["æœˆä»½"]
                    m_df = df[df["æœˆä»½"] == m]
                    matched_contents = [t for t in m_df["å…§å®¹"] if keyword in t]
                    with st.expander(f"ğŸ“° {m.strftime('%Y-%m')} å‡ºç¾ {keyword} çš„æ–°èå…§å®¹ï¼ˆå…± {len(matched_contents)} ç­†ï¼‰"):
                        for t in matched_contents:
                            st.markdown(f"- {t}")
    else:
        st.info("è«‹ä¸Šå‚³è‡³å°‘ä¸€å€‹ Word æª”æ¡ˆä»¥é€²è¡Œåˆ†æã€‚")
